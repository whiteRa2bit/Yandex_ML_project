{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in test dataset: 83194\n"
     ]
    }
   ],
   "source": [
    "train_file = open(\"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/train.txt\",\"r\")\n",
    "train = train_file.read()\n",
    "train = train.split('\\n')\n",
    "train = train[:-1]\n",
    "print(\"Examples in test dataset:\", len(train))\n",
    "x_train = [data_example.split(' ')[0] for data_example in train]\n",
    "y_train = [data_example.split(' ')[1] for data_example in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_copy[:80000]\n",
    "y_train = y_train_copy[:80000]\n",
    "x_test = x_train_copy[80000:]\n",
    "y_test = y_train_copy[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonetic_dict(x, y):\n",
    "    phonetic_dict = {}\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in phonetic_dict:\n",
    "            phonetic_dict[x[i]] = []\n",
    "        phonetic_dict[x[i]].append(y[i])\n",
    "    return phonetic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonetic_dict = get_phonetic_dict(x_train, y_train)\n",
    "example_count = np.sum([len(prons) for _, prons in phonetic_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFECTED --> P_ER_F_EH_K_T_AH_D\n",
      "MORRISSETTE --> M_AO_R_IH_S_EH_T\n",
      "GILLON --> G_IH_L_AH_N\n",
      "GEORGIO --> JH_AO_R_JH_IY_OW\n",
      "SIDEWISE --> S_AY_D_W_AY_Z\n",
      "PETWAY --> P_EH_T_W_EY\n",
      "DISPUTES --> D_IH_S_P_Y_UW_T_S\n",
      "PALKA --> P_AE_L_K_AH\n",
      "DONLIN --> D_AA_N_L_IH_N\n",
      "ZANCA --> Z_AE_NG_K_AH\n",
      "\n",
      "After cleaning, the dictionary contains 80000 words and 80000 pronunciations (0 are alternate pronunciations).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([k+' --> '+phonetic_dict[k][0] for k in random.sample(list(phonetic_dict.keys()), 10)]))\n",
    "print('\\nAfter cleaning, the dictionary contains %s words and %s pronunciations (%s are alternate pronunciations).' % \n",
    "      (len(phonetic_dict), example_count, (example_count-len(phonetic_dict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char to id mapping: \n",
      " {'L': 0, 'E': 1, 'M': 2, 'I': 3, 'U': 4, 'X': 5, 'N': 6, 'D': 7, 'G': 8, 'S': 9, 'T': 10, 'R': 11, 'P': 12, 'K': 13, 'C': 14, 'O': 15, 'F': 16, 'A': 17, 'B': 18, 'H': 19, 'V': 20, 'Y': 21, 'W': 22, 'J': 23, \"'\": 24, 'Q': 25, 'Z': 26, '-': 27}\n",
      "Phone to id mapping: \n",
      " {'': 0, 's': 1, 'e': 2, 'L': 3, 'AH': 4, 'M': 5, 'Y': 6, 'UW': 7, 'AY': 8, 'N': 9, 'D': 10, 'IH': 11, 'NG': 12, 'S': 13, 'T': 14, 'R': 15, 'P': 16, 'K': 17, 'EH': 18, 'AA': 19, 'F': 20, 'ER': 21, 'EY': 22, 'AE': 23, 'Z': 24, 'G': 25, 'B': 26, 'SH': 27, 'V': 28, 'OW': 29, 'AO': 30, 'IY': 31, 'W': 32, 'HH': 33, 'JH': 34, 'CH': 35, 'TH': 36, 'AW': 37, 'OY': 38, 'UH': 39, 'ZH': 40, 'DH': 41}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "START_PHONE_SYM = 's'\n",
    "END_PHONE_SYM = 'e'\n",
    "\n",
    "\n",
    "def char_list():\n",
    "    allowed_symbols = []\n",
    "    for word in x_train:\n",
    "        for char in word:\n",
    "            if char not in allowed_symbols:\n",
    "                allowed_symbols.append(char)\n",
    "    return allowed_symbols\n",
    "\n",
    "\n",
    "def phone_list():\n",
    "    phone_list = [START_PHONE_SYM, END_PHONE_SYM]\n",
    "    for transcription in y_train:\n",
    "        for phone in transcription.split('_'):\n",
    "            if phone not in phone_list:\n",
    "                phone_list.append(phone)\n",
    "    return [''] + phone_list\n",
    "\n",
    "\n",
    "def id_mappings_from_list(str_list):\n",
    "    str_to_id = {s: i for i, s in enumerate(str_list)} \n",
    "    id_to_str = {i: s for i, s in enumerate(str_list)}\n",
    "    return str_to_id, id_to_str\n",
    "\n",
    "\n",
    "# Create character to ID mappings\n",
    "char_to_id, id_to_char = id_mappings_from_list(char_list())\n",
    "\n",
    "# Load phonetic symbols and create ID mappings\n",
    "phone_to_id, id_to_phone = id_mappings_from_list(phone_list())\n",
    "\n",
    "# Example:\n",
    "print('Char to id mapping: \\n', char_to_id)\n",
    "print('Phone to id mapping: \\n', phone_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A\" is represented by:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] \n",
      "-----\n",
      "\"AH\" is represented by:\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "CHAR_TOKEN_COUNT = len(char_to_id)\n",
    "PHONE_TOKEN_COUNT = len(phone_to_id)\n",
    "\n",
    "\n",
    "def char_to_1_hot(char):\n",
    "    char_id = char_to_id[char]\n",
    "    hot_vec = np.zeros((CHAR_TOKEN_COUNT))\n",
    "    hot_vec[char_id] = 1.\n",
    "    return hot_vec\n",
    "\n",
    "\n",
    "def phone_to_1_hot(phone):\n",
    "    phone_id = phone_to_id[phone]\n",
    "    hot_vec = np.zeros((PHONE_TOKEN_COUNT))\n",
    "    hot_vec[phone_id] = 1.\n",
    "    return hot_vec\n",
    "\n",
    "# Example:\n",
    "print('\"A\" is represented by:\\n', char_to_1_hot('A'), '\\n-----')\n",
    "print('\"AH\" is represented by:\\n', phone_to_1_hot('AH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Matrix Shape:  (80000, 34, 28)\n",
      "Pronunciation Matrix Shape:  (80000, 34, 42)\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_SEQ_LEN = max([len(word) for word, _ in phonetic_dict.items()])\n",
    "MAX_PHONE_SEQ_LEN = max([max([len(pron.split('_')) for pron in pronuns]) \n",
    "                         for _, pronuns in phonetic_dict.items()]\n",
    "                       ) + 2  # + 2 to account for the start & end tokens we need to add\n",
    "\n",
    "\n",
    "def dataset_to_1_hot_tensors():\n",
    "    char_seqs = []\n",
    "    phone_seqs = []\n",
    "    \n",
    "    for word, pronuns in phonetic_dict.items():\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "        for t, char in enumerate(word):\n",
    "            word_matrix[t, :] = char_to_1_hot(char)\n",
    "        for pronun in pronuns:\n",
    "            pronun_matrix = np.zeros((MAX_PHONE_SEQ_LEN, PHONE_TOKEN_COUNT))\n",
    "            phones = [START_PHONE_SYM] + pronun.split('_') + [END_PHONE_SYM]\n",
    "            for t, phone in enumerate(phones):\n",
    "                pronun_matrix[t,:] = phone_to_1_hot(phone)\n",
    "                \n",
    "            char_seqs.append(word_matrix)\n",
    "            phone_seqs.append(pronun_matrix)\n",
    "    \n",
    "    return np.array(char_seqs), np.array(phone_seqs)\n",
    "            \n",
    "\n",
    "char_seq_matrix, phone_seq_matrix = dataset_to_1_hot_tensors()        \n",
    "print('Word Matrix Shape: ', char_seq_matrix.shape)\n",
    "print('Pronunciation Matrix Shape: ', phone_seq_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_seq_matrix_decoder_output = np.pad(phone_seq_matrix,((0,0),(0,1),(0,0)), mode='constant')[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "def baseline_model(hidden_nodes = 256):\n",
    "    \n",
    "    # Shared Components - Encoder\n",
    "    char_inputs = Input(shape=(None, CHAR_TOKEN_COUNT))\n",
    "    encoder = LSTM(hidden_nodes, return_state=True)\n",
    "    \n",
    "    # Shared Components - Decoder\n",
    "    phone_inputs = Input(shape=(None, PHONE_TOKEN_COUNT))\n",
    "    decoder = LSTM(hidden_nodes, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    \n",
    "    # Training Model\n",
    "    _, state_h, state_c = encoder(char_inputs) # notice encoder outputs are ignored\n",
    "    encoder_states = [state_h, state_c]\n",
    "    decoder_outputs, _, _ = decoder(phone_inputs, initial_state=encoder_states)\n",
    "    phone_prediction = decoder_dense(decoder_outputs)\n",
    "\n",
    "    training_model = Model([char_inputs, phone_inputs], phone_prediction)\n",
    "    \n",
    "    # Testing Model - Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_states)\n",
    "    \n",
    "    # Testing Model - Decoder\n",
    "    decoder_state_input_h = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder(phone_inputs, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    phone_prediction = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    testing_decoder_model = Model([phone_inputs] + decoder_state_inputs, [phone_prediction] + decoder_states)\n",
    "    \n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0\n",
    "    \n",
    "(char_input_train, char_input_test, \n",
    " phone_input_train, phone_input_test, \n",
    " phone_output_train, phone_output_test) = train_test_split(\n",
    "    char_seq_matrix, phone_seq_matrix, phone_seq_matrix_decoder_output, \n",
    "    test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "TEST_EXAMPLE_COUNT = char_input_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def train(model, weights_path, encoder_input, decoder_input, decoder_output):\n",
    "    checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
    "    stopper = EarlyStopping(monitor='val_loss',patience=3)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit([encoder_input, decoder_input], decoder_output,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          validation_split=0.2, # Keras will automatically create a validation set for us\n",
    "          callbacks=[checkpointer, stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "11776/48000 [======>.......................] - ETA: 1:39 - loss: 0.7108"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c426e64a104f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_encoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_decoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASELINE_MODEL_WEIGHTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_output_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-215f141d0f7e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, weights_path, encoder_input, decoder_input, decoder_output)\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Keras will automatically create a validation set for us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           callbacks=[checkpointer, stopper])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BASELINE_MODEL_WEIGHTS = \"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/baseline_model_weights.hdf5\"\n",
    "training_model, testing_encoder_model, testing_decoder_model = baseline_model()\n",
    "\n",
    "train(training_model, BASELINE_MODEL_WEIGHTS, char_input_train, phone_input_train, phone_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_baseline(input_char_seq, encoder, decoder):\n",
    "    state_vectors = encoder.predict(input_char_seq) \n",
    "    \n",
    "    prev_phone = np.zeros((1, 1, PHONE_TOKEN_COUNT))\n",
    "    prev_phone[0, 0, phone_to_id[START_PHONE_SYM]] = 1.\n",
    "    \n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([prev_phone] + state_vectors)\n",
    "        \n",
    "        # Predict the phoneme with the highest probability\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0, -1, :])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        \n",
    "        pronunciation += predicted_phone + '_'\n",
    "        \n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split('_')) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        \n",
    "        # Setup inputs for next time step\n",
    "        prev_phone = np.zeros((1, 1, PHONE_TOKEN_COUNT))\n",
    "        prev_phone[0, 0, predicted_phone_idx] = 1.\n",
    "        state_vectors = [h, c]\n",
    "        \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for converting vector representations back into words\n",
    "def one_hot_matrix_to_word(char_seq):\n",
    "    word = ''\n",
    "    for char_vec in char_seq[0]:\n",
    "        if np.count_nonzero(char_vec) == 0:\n",
    "            break\n",
    "        hot_bit_idx = np.argmax(char_vec)\n",
    "        char = id_to_char[hot_bit_idx]\n",
    "        word += char\n",
    "    return word\n",
    "\n",
    "\n",
    "# Some words have multiple correct pronunciations\n",
    "# If a prediction matches any correct pronunciation, consider it correct.\n",
    "def is_correct(word,test_pronunciation):\n",
    "    correct_pronuns = phonetic_dict[word]\n",
    "    for correct_pronun in correct_pronuns:\n",
    "        if test_pronunciation == correct_pronun:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def sample_baseline_predictions(sample_count, word_decoder):\n",
    "    TEST_EXAMPLE_COUNT = char_input_test.shape[0]\n",
    "    \n",
    "    sample_indices = random.sample(range(TEST_EXAMPLE_COUNT), sample_count)\n",
    "    counter = 0\n",
    "    iter_num = 0\n",
    "    for example_idx in sample_indices:\n",
    "        example_char_seq = char_input_test[example_idx:example_idx+1]\n",
    "        predicted_pronun = predict_baseline(example_char_seq, testing_encoder_model, testing_decoder_model)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        pred_is_correct = is_correct(example_word, predicted_pronun)\n",
    "        \n",
    "        if predicted_pronun[:-3] == phonetic_dict[example_word][0]:\n",
    "            counter += 1\n",
    "        '''\n",
    "        print('✅ ' if predicted_pronun[:-3] == phonetic_dict[example_word][0] else '❌ ')\n",
    "        print(\"Word:\", example_word)\n",
    "        print(\"Transcription:\", phonetic_dict[example_word][0])\n",
    "        print(\"Prediction:\", predicted_pronun[:-3])\n",
    "        print()\n",
    "        '''\n",
    "        iter_num += 1\n",
    "        if iter_num % 100 == 0:\n",
    "            print(iter_num, \"/\", sample_count)\n",
    "    print(\"Accuracy = \", counter/sample_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x_data, y_data):\n",
    "    char_seqs = []\n",
    "    \n",
    "    for word in x_data:\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "        for t, char in enumerate(word):\n",
    "            word_matrix[t, :] = char_to_1_hot(char)\n",
    "        char_seqs.append(word_matrix)\n",
    "\n",
    "    char_seq_matrix_test = np.array(char_seqs)\n",
    "    \n",
    "    y_predicted = []\n",
    "    for i in range(len(char_seq_matrix_test)):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, '/', len(char_seq_matrix_test))\n",
    "        example_char_seq = char_seq_matrix_test[i:i+1]\n",
    "        predicted_pronun = predict_baseline(example_char_seq, testing_encoder_model, testing_decoder_model)\n",
    "        predicted_pronun = predicted_pronun[:-3]\n",
    "        y_predicted.append(predicted_pronun)\n",
    "        \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_data)):\n",
    "        if y_data[i] == y_predicted[i]:\n",
    "            correct_num += 1\n",
    "    \n",
    "    print(\"Prediction finished!!!\")\n",
    "    return correct_num/len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model, testing_encoder_model, testing_decoder_model = baseline_model()\n",
    "BASELINE_MODEL_WEIGHTS = \"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/baseline_model_weights.hdf5\"\n",
    "training_model.load_weights(BASELINE_MODEL_WEIGHTS)  # also loads weights for testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Prediction finished!!!\n",
      "Accuray on train: 0.6247\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on train:\", get_accuracy(x_train[:10000], y_train[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 3194\n",
      "1000 / 3194\n",
      "2000 / 3194\n",
      "3000 / 3194\n",
      "Prediction finished!!!\n",
      "Accuracy on test: 0.4542892924232937\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test:\", get_accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's count accuracy on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model, testing_encoder_model, testing_decoder_model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_WEIGHTS = \"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/baseline_model_weights.hdf5\"\n",
    "training_model.load_weights(BASELINE_MODEL_WEIGHTS)  # also loads weights for testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train_copy[60000:]\n",
    "y_test = y_train_copy[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_seqs = []\n",
    "    \n",
    "for word in x_test:\n",
    "    word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "    for t, char in enumerate(word):\n",
    "        word_matrix[t, :] = char_to_1_hot(char)\n",
    "    char_seqs.append(word_matrix)\n",
    "\n",
    "char_seq_matrix_test = np.array(char_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 23194\n",
      "1000 / 23194\n",
      "2000 / 23194\n",
      "3000 / 23194\n",
      "4000 / 23194\n",
      "5000 / 23194\n",
      "6000 / 23194\n",
      "7000 / 23194\n",
      "8000 / 23194\n",
      "9000 / 23194\n",
      "10000 / 23194\n",
      "11000 / 23194\n",
      "12000 / 23194\n",
      "13000 / 23194\n",
      "14000 / 23194\n",
      "15000 / 23194\n",
      "16000 / 23194\n",
      "17000 / 23194\n",
      "18000 / 23194\n",
      "19000 / 23194\n",
      "20000 / 23194\n",
      "21000 / 23194\n",
      "22000 / 23194\n",
      "23000 / 23194\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "for i in range(len(char_seq_matrix_test)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i, '/', len(char_seq_matrix_test))\n",
    "    example_char_seq = char_seq_matrix_test[i:i+1]\n",
    "    predicted_pronun = predict_baseline(example_char_seq, testing_encoder_model, testing_decoder_model)\n",
    "    predicted_pronun = predicted_pronun[:-3]\n",
    "    y_predicted.append(predicted_pronun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test =  0.45235836854358885\n"
     ]
    }
   ],
   "source": [
    "correct_num = 0\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] == y_predicted[i]:\n",
    "        correct_num += 1\n",
    "\n",
    "print(\"Accuracy on test = \", correct_num/len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 10000\n",
      "200 / 10000\n",
      "300 / 10000\n",
      "400 / 10000\n",
      "500 / 10000\n",
      "600 / 10000\n",
      "700 / 10000\n",
      "800 / 10000\n",
      "900 / 10000\n",
      "1000 / 10000\n",
      "1100 / 10000\n",
      "1200 / 10000\n",
      "1300 / 10000\n",
      "1400 / 10000\n",
      "1500 / 10000\n",
      "1600 / 10000\n",
      "1700 / 10000\n",
      "1800 / 10000\n",
      "1900 / 10000\n",
      "2000 / 10000\n",
      "2100 / 10000\n",
      "2200 / 10000\n",
      "2300 / 10000\n",
      "2400 / 10000\n",
      "2500 / 10000\n",
      "2600 / 10000\n",
      "2700 / 10000\n",
      "2800 / 10000\n",
      "2900 / 10000\n",
      "3000 / 10000\n",
      "3100 / 10000\n",
      "3200 / 10000\n",
      "3300 / 10000\n",
      "3400 / 10000\n",
      "3500 / 10000\n",
      "3600 / 10000\n",
      "3700 / 10000\n",
      "3800 / 10000\n",
      "3900 / 10000\n",
      "4000 / 10000\n",
      "4100 / 10000\n",
      "4200 / 10000\n",
      "4300 / 10000\n",
      "4400 / 10000\n",
      "4500 / 10000\n",
      "4600 / 10000\n",
      "4700 / 10000\n",
      "4800 / 10000\n",
      "4900 / 10000\n",
      "5000 / 10000\n",
      "5100 / 10000\n",
      "5200 / 10000\n",
      "5300 / 10000\n",
      "5400 / 10000\n",
      "5500 / 10000\n",
      "5600 / 10000\n",
      "5700 / 10000\n",
      "5800 / 10000\n",
      "5900 / 10000\n",
      "6000 / 10000\n",
      "6100 / 10000\n",
      "6200 / 10000\n",
      "6300 / 10000\n",
      "6400 / 10000\n",
      "6500 / 10000\n",
      "6600 / 10000\n",
      "6700 / 10000\n",
      "6800 / 10000\n",
      "6900 / 10000\n",
      "7000 / 10000\n",
      "7100 / 10000\n",
      "7200 / 10000\n",
      "7300 / 10000\n",
      "7400 / 10000\n",
      "7500 / 10000\n",
      "7600 / 10000\n",
      "7700 / 10000\n",
      "7800 / 10000\n",
      "7900 / 10000\n",
      "8000 / 10000\n",
      "8100 / 10000\n",
      "8200 / 10000\n",
      "8300 / 10000\n",
      "8400 / 10000\n",
      "8500 / 10000\n",
      "8600 / 10000\n",
      "8700 / 10000\n",
      "8800 / 10000\n",
      "8900 / 10000\n",
      "9000 / 10000\n",
      "9100 / 10000\n",
      "9200 / 10000\n",
      "9300 / 10000\n",
      "9400 / 10000\n",
      "9500 / 10000\n",
      "9600 / 10000\n",
      "9700 / 10000\n",
      "9800 / 10000\n",
      "9900 / 10000\n",
      "10000 / 10000\n",
      "Accuracy =  0.6281\n"
     ]
    }
   ],
   "source": [
    "training_model.load_weights(BASELINE_MODEL_WEIGHTS)  # also loads weights for testing models\n",
    "sample_baseline_predictions(len(char_input_test), one_hot_matrix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PITCHED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>DISSOLVERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SCRAWNY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BONENFANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>EXCEEDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id        Word\n",
       "0   1     PITCHED\n",
       "1   2  DISSOLVERS\n",
       "2   3     SCRAWNY\n",
       "3   4   BONENFANT\n",
       "4   5     EXCEEDS"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/test.csv\")\n",
    "x_test = list(test_data['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_seqs = []\n",
    "    \n",
    "for word in x_test:\n",
    "    word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "    for t, char in enumerate(word):\n",
    "        word_matrix[t, :] = char_to_1_hot(char)\n",
    "    char_seqs.append(word_matrix)\n",
    "\n",
    "char_seq_matrix_test = np.array(char_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41597\n",
      "(41597, 34, 28)\n"
     ]
    }
   ],
   "source": [
    "print(len(char_seq_matrix_test))\n",
    "print(char_seq_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 41597\n",
      "1000 / 41597\n",
      "2000 / 41597\n",
      "3000 / 41597\n",
      "4000 / 41597\n",
      "5000 / 41597\n",
      "6000 / 41597\n",
      "7000 / 41597\n",
      "8000 / 41597\n",
      "9000 / 41597\n",
      "10000 / 41597\n",
      "11000 / 41597\n",
      "12000 / 41597\n",
      "13000 / 41597\n",
      "14000 / 41597\n",
      "15000 / 41597\n",
      "16000 / 41597\n",
      "17000 / 41597\n",
      "18000 / 41597\n",
      "19000 / 41597\n",
      "20000 / 41597\n",
      "21000 / 41597\n",
      "22000 / 41597\n",
      "23000 / 41597\n",
      "24000 / 41597\n",
      "25000 / 41597\n",
      "26000 / 41597\n",
      "27000 / 41597\n",
      "28000 / 41597\n",
      "29000 / 41597\n",
      "30000 / 41597\n",
      "31000 / 41597\n",
      "32000 / 41597\n",
      "33000 / 41597\n",
      "34000 / 41597\n",
      "35000 / 41597\n",
      "36000 / 41597\n",
      "37000 / 41597\n",
      "38000 / 41597\n",
      "39000 / 41597\n",
      "40000 / 41597\n",
      "41000 / 41597\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for i in range(len(char_seq_matrix_test)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i, '/', len(char_seq_matrix_test))\n",
    "    example_char_seq = char_seq_matrix_test[i:i+1]\n",
    "    predicted_pronun = predict_baseline(example_char_seq, testing_encoder_model, testing_decoder_model)\n",
    "    predicted_pronun = predicted_pronun[:-3]\n",
    "    y_test.append(predicted_pronun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PITCHED', 'DISSOLVERS', 'SCRAWNY', 'BONENFANT', 'EXCEEDS', 'BARTNICKI', 'BUTE', 'CAPITULATE', 'STEAM', 'INVESTCORP']\n",
      "['P_IH_CH_T', 'D_IH_S_AA_L_V_ER_Z', 'S_K_R_AO_N_IY', 'B_AA_N_AH_N_F_AE_N_T', 'IH_K_S_EH_S_T_IH_D', 'B_AA_R_T_N_IH_K_S', 'B_Y_UW_T', 'K_AE_P_IH_T_UW_L_EY_T', 'S_T_IY_M', 'IH_N_V_EH_S_T_P_R_AE_K_T']\n"
     ]
    }
   ],
   "source": [
    "print(x_test[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/test.csv\")\n",
    "submission['Word'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                  Word\n",
      "0   1             P_IH_CH_T\n",
      "1   2    D_IH_S_AA_L_V_ER_Z\n",
      "2   3         S_K_R_AO_N_IY\n",
      "3   4  B_AA_N_AH_N_F_AE_N_T\n",
      "4   5    IH_K_S_EH_S_T_IH_D\n"
     ]
    }
   ],
   "source": [
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"/home/pavel/MyDocs/MachineLearning/Yandex_ML_project/lecture4/Kaggle_phonetics/submission_baseline_model2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    \n",
    "def bleu_score(word,test_pronunciation):\n",
    "    references = [pronun.split('_') for pronun in phonetic_dict[word]]\n",
    "    smooth = SmoothingFunction().method1\n",
    "    return sentence_bleu(references, test_pronunciation.split('_'), smoothing_function=smooth)\n",
    "\n",
    "\n",
    "def evaluate(test_examples, encoder, decoder, word_decoder, predictor):\n",
    "    perfect_predictions = 0\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for example_idx in range(TEST_EXAMPLE_COUNT):\n",
    "        example_char_seq = test_examples[example_idx:example_idx+1]\n",
    "        predicted_pronun = predictor(example_char_seq, encoder, decoder)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        \n",
    "        perfect_predictions += is_correct(example_word,predicted_pronun)\n",
    "\n",
    "        bleu = bleu_score(example_word,predicted_pronun)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "    perfect_acc = perfect_predictions / TEST_EXAMPLE_COUNT\n",
    "    avg_bleu_score = np.mean(bleu_scores)\n",
    "    \n",
    "    return perfect_acc, avg_bleu_score\n",
    "\n",
    "\n",
    "def print_results(model_name, perfect_acc, avg_bleu_score):\n",
    "    print(model_name)\n",
    "    print('-'*20)\n",
    "    print('Perfect Accuracy: %s%%' % round(perfect_acc*100, 1))\n",
    "    print('Bleu Score: %s' % round(avg_bleu_score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model\n",
      "--------------------\n",
      "Perfect Accuracy: 0.0%\n",
      "Bleu Score: 0.3248\n"
     ]
    }
   ],
   "source": [
    "perfect_acc, avg_bleu_score = evaluate(\n",
    "    char_input_test, testing_encoder_model, testing_decoder_model, one_hot_matrix_to_word, predict_baseline)\n",
    "print_results('Baseline Model',perfect_acc, avg_bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
